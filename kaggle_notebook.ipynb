{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba251ed",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "This will install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d06895",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai-whisper librosa soundfile language-tool-python textstat pyyaml numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2e9a5",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea021ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import whisper\n",
    "import librosa\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14139b3d",
   "metadata": {},
   "source": [
    "## Step 3: Audio Processor Class\n",
    "\n",
    "Handles loading and preprocessing of audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"Process audio files for speech-to-text transcription.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate: int = 16000, max_duration: int = 300):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_duration = max_duration\n",
    "    \n",
    "    def load_audio(self, audio_path: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Load audio file from path.\"\"\"\n",
    "        if not os.path.exists(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "        \n",
    "        audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)\n",
    "        return audio, sr\n",
    "    \n",
    "    def preprocess_audio(self, audio: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess audio data.\"\"\"\n",
    "        # Trim silence\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "        \n",
    "        # Limit duration\n",
    "        max_samples = self.sample_rate * self.max_duration\n",
    "        if len(audio) > max_samples:\n",
    "            audio = audio[:max_samples]\n",
    "        \n",
    "        # Normalize\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        return audio\n",
    "    \n",
    "    def process_audio_file(self, audio_path: str) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"Complete processing pipeline for an audio file.\"\"\"\n",
    "        audio, sr = self.load_audio(audio_path)\n",
    "        \n",
    "        duration = len(audio) / sr\n",
    "        info = {\n",
    "            'path': str(audio_path),\n",
    "            'sample_rate': sr,\n",
    "            'duration_seconds': duration,\n",
    "            'samples': len(audio)\n",
    "        }\n",
    "        \n",
    "        processed_audio = self.preprocess_audio(audio)\n",
    "        info['processed_duration'] = len(processed_audio) / sr\n",
    "        \n",
    "        return processed_audio, info\n",
    "\n",
    "print(\"‚úì AudioProcessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2717d",
   "metadata": {},
   "source": [
    "## Step 4: Transcriber Class\n",
    "\n",
    "Converts speech to text using OpenAI Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcriber:\n",
    "    \"\"\"Transcribe audio to text using Whisper.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"base\", device: str = \"cpu\", language: str = \"en\"):\n",
    "        self.model_name = model_name\n",
    "        self.language = language\n",
    "        self.device = device\n",
    "        \n",
    "        print(f\"Loading Whisper model '{model_name}' on {device}...\")\n",
    "        self.model = whisper.load_model(model_name, device=device)\n",
    "        print(\"‚úì Model loaded successfully!\")\n",
    "    \n",
    "    def transcribe(self, audio: np.ndarray) -> str:\n",
    "        \"\"\"Transcribe audio array to text.\"\"\"\n",
    "        result = self.model.transcribe(audio, language=self.language, fp16=False)\n",
    "        return result['text'].strip()\n",
    "    \n",
    "    def transcribe_file(self, audio_path: str) -> Dict:\n",
    "        \"\"\"Transcribe audio file and return detailed results.\"\"\"\n",
    "        audio_processor = AudioProcessor()\n",
    "        audio, info = audio_processor.process_audio_file(audio_path)\n",
    "        \n",
    "        text = self.transcribe(audio)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'file_path': str(audio_path),\n",
    "            'file_name': Path(audio_path).name,\n",
    "            'model': self.model_name,\n",
    "            'word_count': len(text.split()),\n",
    "            'duration': info['duration_seconds']\n",
    "        }\n",
    "\n",
    "print(\"‚úì Transcriber class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf7337",
   "metadata": {},
   "source": [
    "## Step 5: Grammar Scorer Class\n",
    "\n",
    "Analyzes text for grammar quality and provides scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarScorer:\n",
    "    \"\"\"Analyze text for grammar quality and provide detailed scoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, language: str = \"en-US\", use_language_tool: bool = True):\n",
    "        self.language = language\n",
    "        self.use_language_tool = use_language_tool\n",
    "        self.grammar_tool = None\n",
    "        \n",
    "        if use_language_tool:\n",
    "            try:\n",
    "                import language_tool_python\n",
    "                print(f\"Initializing LanguageTool for {language}...\")\n",
    "                self.grammar_tool = language_tool_python.LanguageTool(language)\n",
    "                print(\"‚úì LanguageTool initialized!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not initialize LanguageTool: {e}\")\n",
    "                self.use_language_tool = False\n",
    "    \n",
    "    def check_grammar(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Check text for grammar errors.\"\"\"\n",
    "        if not self.use_language_tool or self.grammar_tool is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            matches = self.grammar_tool.check(text)\n",
    "            errors = []\n",
    "            for match in matches:\n",
    "                errors.append({\n",
    "                    'message': match.message,\n",
    "                    'category': match.category,\n",
    "                    'suggestions': match.replacements[:3],\n",
    "                    'context': match.context\n",
    "                })\n",
    "            return errors\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def analyze_vocabulary(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze vocabulary richness.\"\"\"\n",
    "        import re\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        \n",
    "        if not words:\n",
    "            return {'word_count': 0, 'unique_words': 0, 'lexical_diversity': 0.0}\n",
    "        \n",
    "        unique_words = set(words)\n",
    "        lexical_diversity = len(unique_words) / len(words)\n",
    "        \n",
    "        return {\n",
    "            'word_count': len(words),\n",
    "            'unique_words': len(unique_words),\n",
    "            'lexical_diversity': lexical_diversity,\n",
    "            'avg_word_length': np.mean([len(w) for w in words])\n",
    "        }\n",
    "    \n",
    "    def analyze_sentence_structure(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze sentence structure.\"\"\"\n",
    "        import re\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if not sentences:\n",
    "            return {'sentence_count': 0, 'avg_sentence_length': 0}\n",
    "        \n",
    "        sentence_lengths = [len(s.split()) for s in sentences]\n",
    "        \n",
    "        return {\n",
    "            'sentence_count': len(sentences),\n",
    "            'avg_sentence_length': np.mean(sentence_lengths),\n",
    "            'max_sentence_length': max(sentence_lengths),\n",
    "            'min_sentence_length': min(sentence_lengths)\n",
    "        }\n",
    "    \n",
    "    def calculate_readability(self, text: str) -> Dict:\n",
    "        \"\"\"Calculate readability scores.\"\"\"\n",
    "        try:\n",
    "            import textstat\n",
    "            flesch_score = textstat.flesch_reading_ease(text)\n",
    "            return {'flesch_reading_ease': max(0, min(100, flesch_score))}\n",
    "        except:\n",
    "            # Simplified calculation if textstat fails\n",
    "            import re\n",
    "            sentences = re.split(r'[.!?]+', text)\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "            words = text.split()\n",
    "            \n",
    "            if not sentences or not words:\n",
    "                return {'flesch_reading_ease': 50}\n",
    "            \n",
    "            avg_words = len(words) / len(sentences)\n",
    "            score = max(0, min(100, 100 - abs(avg_words - 15) * 2))\n",
    "            return {'flesch_reading_ease': score}\n",
    "    \n",
    "    def score_text(self, text: str, weights: Optional[Dict] = None) -> Dict:\n",
    "        \"\"\"Comprehensive grammar scoring of text.\"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return {'score': 0.0, 'grade': 'N/A', 'error': 'Empty text'}\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = {\n",
    "                'grammar_errors': 0.40,\n",
    "                'sentence_structure': 0.20,\n",
    "                'vocabulary_richness': 0.20,\n",
    "                'readability': 0.20\n",
    "            }\n",
    "        \n",
    "        # Check grammar\n",
    "        grammar_errors = self.check_grammar(text)\n",
    "        vocab_analysis = self.analyze_vocabulary(text)\n",
    "        sentence_analysis = self.analyze_sentence_structure(text)\n",
    "        readability = self.calculate_readability(text)\n",
    "        \n",
    "        # Calculate component scores\n",
    "        word_count = vocab_analysis['word_count']\n",
    "        error_rate = len(grammar_errors) / max(word_count, 1) * 100\n",
    "        grammar_score = max(0, 100 - error_rate * 10)\n",
    "        \n",
    "        avg_length = sentence_analysis['avg_sentence_length']\n",
    "        structure_score = 100\n",
    "        if avg_length < 5:\n",
    "            structure_score = 60\n",
    "        elif avg_length > 30:\n",
    "            structure_score = 70\n",
    "        \n",
    "        lexical_div = vocab_analysis['lexical_diversity']\n",
    "        vocab_score = min(100, lexical_div * 200)\n",
    "        \n",
    "        readability_score = readability.get('flesch_reading_ease', 50)\n",
    "        \n",
    "        # Calculate final score\n",
    "        final_score = (\n",
    "            grammar_score * weights['grammar_errors'] +\n",
    "            structure_score * weights['sentence_structure'] +\n",
    "            vocab_score * weights['vocabulary_richness'] +\n",
    "            readability_score * weights['readability']\n",
    "        )\n",
    "        \n",
    "        # Determine grade\n",
    "        if final_score >= 90:\n",
    "            grade = 'A (Excellent)'\n",
    "        elif final_score >= 75:\n",
    "            grade = 'B (Good)'\n",
    "        elif final_score >= 60:\n",
    "            grade = 'C (Average)'\n",
    "        elif final_score >= 40:\n",
    "            grade = 'D (Poor)'\n",
    "        else:\n",
    "            grade = 'F (Very Poor)'\n",
    "        \n",
    "        return {\n",
    "            'score': round(final_score, 2),\n",
    "            'grade': grade,\n",
    "            'grammar_errors': grammar_errors,\n",
    "            'error_count': len(grammar_errors),\n",
    "            'sentence_analysis': sentence_analysis,\n",
    "            'vocabulary_analysis': vocab_analysis,\n",
    "            'readability': readability,\n",
    "            'component_scores': {\n",
    "                'grammar': round(grammar_score, 2),\n",
    "                'structure': round(structure_score, 2),\n",
    "                'vocabulary': round(vocab_score, 2),\n",
    "                'readability': round(readability_score, 2)\n",
    "            },\n",
    "            'text': text,\n",
    "            'word_count': word_count\n",
    "        }\n",
    "\n",
    "print(\"‚úì GrammarScorer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4debc",
   "metadata": {},
   "source": [
    "## Step 6: Main Grammar Scoring Engine\n",
    "\n",
    "Combines all components into a complete system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63925632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarScoringEngine:\n",
    "    \"\"\"Complete Grammar Scoring Engine from Voice Samples.\"\"\"\n",
    "    \n",
    "    def __init__(self, whisper_model: str = \"base\"):\n",
    "        print(\"Initializing Grammar Scoring Engine...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.transcriber = Transcriber(model_name=whisper_model)\n",
    "        self.grammar_scorer = GrammarScorer(language=\"en-US\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"‚úì Grammar Scoring Engine initialized!\\n\")\n",
    "    \n",
    "    def score_audio(self, audio_path: str) -> Dict:\n",
    "        \"\"\"Process audio file and score grammar.\"\"\"\n",
    "        print(f\"\\nProcessing: {Path(audio_path).name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Transcribe\n",
    "        print(\"1. Transcribing audio...\")\n",
    "        transcription_result = self.transcriber.transcribe_file(audio_path)\n",
    "        text = transcription_result['text']\n",
    "        print(f\"   Transcribed: {text}\")\n",
    "        print(f\"   Word count: {transcription_result['word_count']}\")\n",
    "        \n",
    "        # Score grammar\n",
    "        print(\"\\n2. Analyzing grammar...\")\n",
    "        scoring_result = self.grammar_scorer.score_text(text)\n",
    "        \n",
    "        # Combine results\n",
    "        complete_result = {\n",
    "            'file_name': Path(audio_path).name,\n",
    "            'file_path': str(audio_path),\n",
    "            'transcription': transcription_result,\n",
    "            **scoring_result\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Overall Score: {scoring_result['score']:.2f}/100\")\n",
    "        print(f\"Grade: {scoring_result['grade']}\")\n",
    "        print(f\"Errors Found: {scoring_result['error_count']}\")\n",
    "        print(f\"Word Count: {scoring_result['word_count']}\")\n",
    "        \n",
    "        if scoring_result['error_count'] > 0:\n",
    "            print(f\"\\nTop Errors:\")\n",
    "            for i, error in enumerate(scoring_result['grammar_errors'][:3], 1):\n",
    "                print(f\"  {i}. {error['message']}\")\n",
    "        \n",
    "        print(\"\\nComponent Scores:\")\n",
    "        for component, score in scoring_result['component_scores'].items():\n",
    "            print(f\"  - {component.capitalize()}: {score:.2f}/100\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return complete_result\n",
    "\n",
    "print(\"‚úì GrammarScoringEngine class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc65b85",
   "metadata": {},
   "source": [
    "## Step 7: Check Available Datasets\n",
    "\n",
    "**Important:** Before running this cell, add a speech dataset using the **\"+ Add Data\"** button on the right sidebar.\n",
    "\n",
    "Recommended datasets:\n",
    "- Search for: \"speech commands\"\n",
    "- Or use any audio dataset with `.wav` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20dbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available datasets in Kaggle input\n",
    "input_dir = '/kaggle/input'\n",
    "\n",
    "if os.path.exists(input_dir):\n",
    "    print(\"Available datasets in Kaggle input:\")\n",
    "    print(\"=\"*60)\n",
    "    for item in os.listdir(input_dir):\n",
    "        dataset_path = os.path.join(input_dir, item)\n",
    "        print(f\"\\nüìÅ {item}\")\n",
    "        \n",
    "        # Show first few items in dataset\n",
    "        if os.path.isdir(dataset_path):\n",
    "            contents = os.listdir(dataset_path)[:5]\n",
    "            for content in contents:\n",
    "                print(f\"   - {content}\")\n",
    "            if len(os.listdir(dataset_path)) > 5:\n",
    "                print(f\"   ... and {len(os.listdir(dataset_path)) - 5} more items\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No input data found!\")\n",
    "    print(\"Please add a dataset using the '+ Add Data' button.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29047790",
   "metadata": {},
   "source": [
    "## Step 8: Initialize the Engine\n",
    "\n",
    "Create an instance of the Grammar Scoring Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the engine\n",
    "# Use 'tiny' for faster processing, 'base' for better accuracy\n",
    "engine = GrammarScoringEngine(whisper_model='base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44591a51",
   "metadata": {},
   "source": [
    "## Step 9: Process a Single Audio File\n",
    "\n",
    "**Update the path below** to match your dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ae5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process one audio file\n",
    "# UPDATE THIS PATH to match your dataset!\n",
    "audio_file = '/kaggle/input/google-speech-commands/cat/004ae714_nohash_0.wav'\n",
    "\n",
    "if os.path.exists(audio_file):\n",
    "    result = engine.score_audio(audio_file)\n",
    "    \n",
    "    # Show full result as JSON\n",
    "    print(\"\\nFull Result (JSON):\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {audio_file}\")\n",
    "    print(\"\\nPlease update the path to match your dataset structure.\")\n",
    "    print(\"Example paths:\")\n",
    "    print(\"  - /kaggle/input/your-dataset-name/audio.wav\")\n",
    "    print(\"  - /kaggle/input/speech-commands/cat/audio.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab3f72",
   "metadata": {},
   "source": [
    "## Step 10: Batch Processing (Optional)\n",
    "\n",
    "Process multiple audio files at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory: str, limit: int = 5):\n",
    "    \"\"\"Process multiple audio files from a directory.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Find audio files\n",
    "    audio_files = list(Path(directory).glob('*.wav'))[:limit]\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(f\"No .wav files found in {directory}\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"\\nProcessing {len(audio_files)} audio files...\\n\")\n",
    "    \n",
    "    for i, audio_file in enumerate(audio_files, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"File {i}/{len(audio_files)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            result = engine.score_audio(str(audio_file))\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {audio_file.name}: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BATCH PROCESSING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        scores = [r['score'] for r in results if 'score' in r]\n",
    "        if scores:\n",
    "            print(f\"Files processed: {len(results)}\")\n",
    "            print(f\"Average score: {sum(scores)/len(scores):.2f}\")\n",
    "            print(f\"Highest score: {max(scores):.2f}\")\n",
    "            print(f\"Lowest score: {min(scores):.2f}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment and update path):\n",
    "# results = process_directory('/kaggle/input/speech-commands/cat', limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0526f",
   "metadata": {},
   "source": [
    "## Step 11: Test with Custom Text (No Audio)\n",
    "\n",
    "You can also test the grammar scorer directly with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test grammar scoring with sample text\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "This is a well-written sentence with proper grammar and punctuation.\n",
    "Machine learning algorithms can process vast amounts of data efficiently.\n",
    "\"\"\"\n",
    "\n",
    "scorer = GrammarScorer()\n",
    "result = scorer.score_text(sample_text)\n",
    "\n",
    "print(\"\\nGrammar Analysis of Sample Text:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Score: {result['score']:.2f}/100\")\n",
    "print(f\"Grade: {result['grade']}\")\n",
    "print(f\"Errors: {result['error_count']}\")\n",
    "print(f\"Words: {result['word_count']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b41ea0",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Add a speech dataset** using the \"+ Add Data\" button\n",
    "2. **Update the audio file paths** in Step 9 to match your dataset\n",
    "3. **Run all cells** sequentially from top to bottom\n",
    "4. **Experiment** with different Whisper models (tiny, base, small)\n",
    "5. **Process multiple files** using the batch processing function in Step 10\n",
    "\n",
    "## Tips\n",
    "\n",
    "- Use `whisper_model='tiny'` for faster processing\n",
    "- Use `whisper_model='small'` or `'base'` for better accuracy\n",
    "- Enable GPU in Settings ‚Üí Accelerator for faster transcription\n",
    "- Single-word audio clips will have limited grammar analysis (expected behavior)\n",
    "\n",
    "## GitHub Repository\n",
    "\n",
    "Full source code: [Add your GitHub URL here after pushing]\n",
    "\n",
    "---\n",
    "\n",
    "**Built with:** OpenAI Whisper, LanguageTool, Librosa"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
